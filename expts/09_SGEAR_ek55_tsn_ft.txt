#LOSS weights
train.train_one_epoch_fn.loss_wts.feat=1.0
train.train_one_epoch_fn.loss_wts.past_cls_action=0.0
train.train_one_epoch_fn.loss_wts.cls_action=1.0
train.train_one_epoch_fn.loss_wts.global_simContrast=2.0
train.train_one_epoch_fn.loss_wts.target_distance=0.0
train.train_one_epoch_fn.loss_wts.pred=0.0
train.train_one_epoch_fn.loss_wts.past_target_global_simContrast=0.0


#Initialization
#train.init_from_model=[[backbone.model,${cwd}/DATA/pretrained/TIMM/jx_vit_base_p16_224-80ecf9dd.pth]]
expt_name=TimeShift_avt_epic55_28_06
test_only=true
pre_train_path=${cwd}/OUTPUTS/expts/09_SGEAR_ek55_tsn.txt/0/checkpoint.pth
+pretrain_blocks=[transform_features,gamma,mapper_to_inter,future_predictor]
pre_train=true


#training setting
train.batch_size=32
eval.batch_size=32
train.num_epochs=20
train.store_best=true

opt.lr_wd=[[__all__,0.00001,0.0001]]
opt.bias_bn_wd_scale=1.0

opt/optimizer=adamW
opt/scheduler=cosine
opt.warmup.num_epochs=5
opt.scheduler.eta_min=5e-09
opt.accumulate=1


#Detector (depracated)
+model.detector=false
+model.detector_dim=2048
+model.freeze_detector=false
+model.pretrained_detector=false
+model.pretrained_detector_weights=${cwd}/DATA/pretrained/TIMM/res_obj_det.pth

#Mixup at raw frame level setting (not recomended)
train.use_mixup=false
train.mixup_alpha=0.8
train.label_smoothing=0.1

#Object extracted_features
+model.use_object=false
+model.inp_multimodal_dim=2400
model.multimodal=false
+model.num_modalities=1
+model.feature_mixup=false
+model.transform_features=true
+model.extracted_features=true


#Freeze weights
+model.freeze_gamma=true
+model.freeze_future_predictor=false
model.freeze_weights=false
+model.freeze_transform_features=true


#Backbone setting
+model.reg=false
model/backbone=identity
model.backbone_last_n_modules_to_drop=0
model.backbone_dim=1024
model.intermediate_featdim=1024
model.cls_token=true
model.backbone_add_ts_blocks=0
+model.mod_attn=false
model/temporal_aggregator=identity


#Temporal encoder
model/future_predictor=avth
+model.future_steps=0
model.dropout=0.8
+model.future_predictor.n_head=16
+model.future_predictor.n_layer=6
+model.future_predictor.output_len=1
+model.future_predictor.inter_dim=2048
+model.future_predictor.return_past_too=true
+model.future_predictor.future_pred_loss={_target_: torch.nn.MSELoss}
+model.future_predictor.future_pred_loss_wt=1.0
+model.future_predictor.avg_last_n=1
model.classifier_on_past=true


#DATA setup
data_train.num_frames=10
model.tmp_frames=${data_train.num_frames}
data_train.frame_rate=1
data_train.subclips.num_frames=1
data_train.subclips.stride=1
data_eval.num_frames=${data_train.num_frames}
data_eval.frame_rate=${data_train.frame_rate}
data_eval.subclips.num_frames=${data_train.subclips.num_frames}
data_eval.subclips.stride=${data_train.subclips.stride}
data_train.mean=[0.5, 0.5, 0.5]
data_train.std=[0.5, 0.5, 0.5]
data_eval.mean=${data_train.mean}
data_eval.std=${data_train.std}
data_eval.eval_num_crops=1
data_eval.eval_flip_crops=false


#Anticipation setup
dataset@dataset_train=epic_kitchens/anticipation_train_minus_val_mm
dataset@dataset_eval=epic_kitchens/anticipation_test_s1
+dataset@dataset_eval_s2=epic_kitchens/anticipation_test_s2
dataset_train.sample_strategy=last_clip
dataset_eval.sample_strategy=last_clip
dataset_eval_s2.sample_strategy=last_clip
dataset_train.conv_to_anticipate_fn.tau_o=5
dataset_eval.conv_to_anticipate_fn.tau_o=5
dataset_eval_s2.conv_to_anticipate_fn.tau_o=5
dataset_train.conv_to_anticipate_fn.tau_a=1
dataset_eval.conv_to_anticipate_fn.tau_a=1
dataset_eval_s2.conv_to_anticipate_fn.tau_a=1
dataset.epic_kitchens.common.label_type=action

+dataset_train.conv_to_anticipate_fn.drop_style=correct
+dataset_eval.conv_to_anticipate_fn.drop_style=correct
+dataset_eval_s2.conv_to_anticipate_fn.drop_style=correct

#Frame scale
data_train.scale_h=248-280
data_train.scale_w=-1
data_train.crop_size=224
data_eval.scale_h=248
data_eval.scale_w=-1
data_eval.crop_size=224

hydra.launcher.nodes=1
hydra.launcher.gpus_per_node=1